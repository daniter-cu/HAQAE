{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "outfile = \"haqae_data.txt\"\n",
    "with open(\"hm_indexed.csv\", 'r') as handle:\n",
    "    with open(outfile, 'w') as out:\n",
    "        reader = csv.reader(handle)\n",
    "        for line in reader:\n",
    "            out.write(line[3] + \" \")\n",
    "            i += 1\n",
    "            if i > 10:\n",
    "                out.write(\"\\n\")\n",
    "                i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files:\n",
    "<TUP> v, s, o, p ... v, s, o, p # <TUP> {} {} {} {} ##\n",
    "<TUP> v_t, s_t, ...\n",
    "4 tuples per line\n",
    "#use null char for prep\n",
    "fix text field for vocab in happydb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchtext.data as ttdata\n",
    "import torchtext.datasets as ttdatasets\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(ttdata.Dataset):\n",
    "    'Data set which has a single sentence per line'\n",
    "\n",
    "    def __init__(self, path, text_field, target_field , src_seq_length=50, min_seq_length=8, n_cloze=False, add_eos=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Args\n",
    "            path (str) : Filename of text file with dataset\n",
    "            vocab (Torchtext Vocab object)\n",
    "            filter_pred (callable) : Only use examples for which filter_pred(example) is TRUE\n",
    "        \"\"\"\n",
    "        #text_field = ExtendableField(vocab)\n",
    "\n",
    "#         if add_eos:\n",
    "#             target_field = ExtendableField(vocab, eos_token=EOS_TOK)\n",
    "#         else:\n",
    "#             target_field = ExtendableField(vocab) # added this for narrative cloze\n",
    "       \n",
    "\n",
    "        fields = [('text', text_field), ('target', target_field)]\n",
    "        examples = []\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                text = line\n",
    "                if n_cloze:\n",
    "                    text = text.split(\"<TUP>\") \n",
    "                    actual_event = text[-1] #last event\n",
    "                    text = text[:-1] # ignore the last tuple\n",
    "                    text = \"<TUP>\".join(text)\n",
    "                    #print(\"cloze text is {}\".format(text))\n",
    "                    # text has t-1 events and target has the t event\n",
    "                    examples.append(ttdata.Example.fromlist([text, actual_event], fields))\n",
    "                else:\n",
    "                    examples.append(ttdata.Example.fromlist([text, text], fields))\n",
    "\n",
    "        def filter_pred(example):\n",
    "            return len(example.text) <= src_seq_length and len(example.text) >= min_seq_length\n",
    " \n",
    "        super(SentenceDataset, self).__init__(examples, fields, filter_pred=filter_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = data.Field(lower=True)\n",
    "label_field = data.Field(sequential=False)\n",
    "dataset = SentenceDataset(\"haqae_formated.txt\", text_field, label_field , 50, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [01:09, 8.10MB/s]                              \n",
      "100%|█████████▉| 398645/400000 [00:16<00:00, 24139.46it/s]"
     ]
    }
   ],
   "source": [
    "text_field.build_vocab(dataset, vectors=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"vocab.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(text_field.vocab, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'batch_first',\n",
       " 'build_vocab',\n",
       " 'dtype',\n",
       " 'dtypes',\n",
       " 'eos_token',\n",
       " 'fix_length',\n",
       " 'include_lengths',\n",
       " 'init_token',\n",
       " 'is_target',\n",
       " 'lower',\n",
       " 'numericalize',\n",
       " 'pad',\n",
       " 'pad_first',\n",
       " 'pad_token',\n",
       " 'postprocessing',\n",
       " 'preprocess',\n",
       " 'preprocessing',\n",
       " 'process',\n",
       " 'sequential',\n",
       " 'stop_words',\n",
       " 'tokenize',\n",
       " 'truncate_first',\n",
       " 'unk_token',\n",
       " 'use_vocab',\n",
       " 'vocab',\n",
       " 'vocab_cls']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
